{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_url = \"https://eprocure.gov.in/cppp/resultoftendersnew/mmpdata/byUVhOellXMD1BMTNoMVpteHZiMlE9QTEzaDFBMTNoMUExM2gxTWpBeU1BPT1BMTNoMU1RPT0=?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = requests.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #attempt 1 - success\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "import lxml.html as lh\n",
    "\n",
    "\n",
    "#COLUMNS = (\"SINO\",\"AOC\",\"epub\", \"title\",\"state\")\n",
    "#f.write(COLUMNS)\n",
    "#put page range and get only table)\n",
    "baseurl = \"https://eprocure.gov.in/cppp/latestactivetendersnew/mmpdata/byYzJWc1pXTjBBMTNoMWMyVnNaV04wQTEzaDFjSFZpYkdsemFHVmtYMlJoZEdVPUExM2gxVGtoTg=?\"\n",
    "for page in range(1,20):\n",
    "        h = {\"Cookie\": \"li=https%3A%2F%2Feprocure.gov.in%2Fcppp%2Fcancelledtenders; has_js=1; cookieWorked=yes; SSESS4e4a4d945e1f90e996acd5fb569779de=XbXDqjylwmrH0pAE-MEF55tkmmoTErEMbY6F3iklDCE\"}\n",
    "        r = requests.get(baseurl +'page='+ str(page),headers = h)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser') \n",
    "        #print(soup)\n",
    "        table = soup.find(\"table\")\n",
    "        table_new = table.findAll('tr')\n",
    "        with open('output.csv', 'a', newline='') as f:\n",
    "                writer = csv.writer(f)\n",
    "                for items in table_new:\n",
    "                        data = [item.get_text(strip=True) for item in items.find_all(['th','td'])]\n",
    "                        print(data)\n",
    "                        writer.writerow(data) \n",
    "        \n",
    "    #possibly to get all the links in the table (need to click on them and return data from in there)\n",
    "    #for anchor in table.findAll('a'):\n",
    "        #if not anchor: finaAll returns empty list, .find() return None\n",
    "            #continue\n",
    "        #href = anchor['href']    \n",
    "        #print (href)\n",
    "        #print (anchor.text)\n",
    "#create a csv and text from tables in\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt 2\n",
    "dataframe = []\n",
    "COLUMNS = ['SINO','AOC', 'epub', 'title', 'state']\n",
    "baseurl = \"https://eprocure.gov.in/cppp/resultoftendersnew/mmpdata/byUVhOellXMD1BMTNoMVpteHZiMlE9QTEzaDFBMTNoMUExM2gxTWpBeU1BPT1BMTNoMU1RPT0=?\"\n",
    "for page in range(1,10):\n",
    "    r = requests.get(baseurl +'page='+ str(page))\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')   \n",
    "    table = soup.find(\"table\", {'class': 'list_table'})# Find the \"table\" tag in the page\n",
    "    tbody = table.find('tbody')\n",
    "    rows = table.find_all(\"tr\") # Find all the \"tr\" tags in the table\n",
    "    cy_data = [] \n",
    "    for row in rows:\n",
    "        cells = row.find_all(\"td\") #  Find all the \"td\" tags in each row \n",
    "        cells = cells[0:5] # Select the correct columns\n",
    "        cy_data.append([cell.text for cell in cells]) # For each \"td\" tag, get the text inside it\n",
    "    dataframes.append(pd.DataFrame(cy_data, columns=COLUMNS).drop(0, axis=0))\n",
    "            \n",
    "#seems to be printing the same ten values again and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attempt3 - active tenders\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "import lxml.html as lh\n",
    "\n",
    "# NOTE: to test from command line, use curl:\n",
    "# curl --cookie \"li=https%3A%2F%2Feprocure.gov.in%2Fcppp%2Fcancelledtenders; has_js=1; cookieWorked=yes; SSESS4e4a4d945e1f90e996acd5fb569779de=XbXDqjylwmrH0pAE-MEF55tkmmoTErEMbY6F3iklDCE\" --referer \"https://eprocure.gov.in/\" https://eprocure.gov.in/cppp/tendersfullviewmmp/Mjc0NDkwNDM=A13h1VmpGYWExTXlTWGxTYkdoUFZqSm9jbHBJYjNkUFVUMDk=A13h1VmpGYWExTXlTWGxTYkdoUFZqSm9jbHBJYjNkUFVUMDk=A13h1MTYxOTczMTgzNw==A13h1c21jL2Nvbi9FVGVuZGVyTm8gMy85LzIwMjEtMjI=A13h1MjAyMV9ETUFfNjgwMzcwXzE=  | grep \"Tender Details\"\n",
    "#COLUMNS = (\"orgname\",\"orgtype\",\"tendertitle\",\"tenderrefno\",\"tendercateg\",\"tendertype\", \"Prodcategory\",\"tenderfee\",\"EMD\",\"Location\",\"Bidopeningdate\", \"docdownloadstart\",\"docdownloadend\",\"Bidsubmissionstart\", \"workdesc\",\"name\",\"address\")\n",
    "#f.write(COLUMNS)\n",
    " #possibly to get all the links in the table (need to click on them and return data from in there)\n",
    "#put page range and get only table\n",
    "#old: \"Cookie\": \"li=https%3A%2F%2Feprocure.gov.in%2Fcppp%2Fcancelledtenders; has_js=1; cookieWorked=yes; SSESS4e4a4d945e1f90e996acd5fb569779de=XbXDqjylwmrH0pAE-MEF55tkmmoTErEMbY6F3iklDCE\n",
    "\n",
    "baseurl = \"https://eprocure.gov.in/cppp/latestactivetendersnew/mmpdata/byYzJWc1pXTjBBMTNoMWMyVnNaV04wQTEzaDFjSFZpYkdsemFHVmtYMlJoZEdVPUExM2gxVGtoTg=?\"\n",
    "\n",
    "for page in range(1,1):\n",
    "        r = requests.get(baseurl +'page='+ str(page))\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        #print(table)\n",
    "        for row in table.find_all(\"tr\")[1:]:\n",
    "            fifth_col = row('td')[4].find('a').get(\"href\", None)\n",
    "            print(fifth_col)\n",
    "            h = {\"Referer\": \"https://eprocure.gov.in/\", \"Cookie\": \"li=https%3A%2F%2Feprocure.gov.in%2Fcppp%2Fcancelledtenders; has_js=1; cookieWorked=yes; SSESS4e4a4d945e1f90e996acd5fb569779de=XbXDqjylwmrH0pAE-MEF55tkmmoTErEMbY6F3iklDCE\"}\n",
    "            inner_response = requests.get(fifth_col, headers=h)\n",
    "            inner_soup = BeautifulSoup(inner_response.content, 'html.parser')\n",
    "            if \"Invalid parameter\" in str(inner_soup):\n",
    "                raise Exception(\"Invalid parameter found. Did you forget to set the cookies?\")\n",
    "                with open(\"soups.html\", \"w\") as text_file:\n",
    "                     print(inner_soup, file=text_file)\n",
    "            div = soup.find(\"div\", {\"id\": \"tfullview\"})\n",
    "            table_inner = div.findAll('table')\n",
    "            \n",
    "            #text = inner_soup.findAll('table', attr = {\"id\":\"tenderDetailDivTd\"})\n",
    "            #print(text)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    #text = inner_soup.findAll(table, attr = {\"id\":\"tenderDetailDivTd\"})\n",
    "    #print(text)\n",
    "    \n",
    "    #inner_table_2 = inner_soup.findAll('table')[1]\n",
    "    #print(inner_table_2)\n",
    "    #inner_table_4 = inner_soup.findAll('table')[3]\n",
    "    #inner_table_6 = inner_soup.findAll('table')[5]\n",
    "    #inner_table_6 = inner_soup.findAll('table')[5]\n",
    "    \n",
    "    #rows = inner_table_2.findAll(\"tr\", attr ={\"id\":\"tenderDetailDivTd\"})\n",
    "    #print(rows)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #create a csv and text from tables in\n",
    "    #with open('output1.csv', 'a', newline='') as f:\n",
    "     #   writer = csv.writer(f)\n",
    "      #  for items in table_new:\n",
    "       #     data = [item.get_text(strip=True) for item in items.find_all(['th','td'])]\n",
    "        #    print(data)\n",
    "         #   writer.writerow(data)       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tag in fifth_col:\n",
    "             #   link = tag.get(\"href\", None)\n",
    "              #  print(link)\n",
    "        \n",
    "        #table_new = table.findAll('tr')\n",
    "        \n",
    "        \n",
    "        #for row in soup.findAll('table'):\n",
    "          #      row = table.findAll('tr')\n",
    "        \n",
    "            #    fifth_column_tags = row.findAll('a')[4].contents\n",
    "        \n",
    "        #a_tags = table.findAll('a')\n",
    "                #print(fifth_column_tags)\n",
    "        #for tag in a_tags:\n",
    "         #   print(tag)\n",
    "          #  link = tag.get(\"href\", None)\n",
    "           # print(link)\n",
    "            \n",
    "#for row in soup.findAll('table')[0].table.findAll('tr'):\n",
    " #   first_column = row.findAll('th')[0].contents\n",
    "  #  third_column = row.findAll('td')[2].contents\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
